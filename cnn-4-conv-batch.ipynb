{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CNN with 4 Convolutional Layers and Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pradeepsingh/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(12345)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images_path = keras.utils.get_file('train-images-idx3-ubyte.gz', 'https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/data/fashion/train-images-idx3-ubyte.gz')\n",
    "train_labels_path = keras.utils.get_file('train-labels-idx1-ubyte.gz', 'https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/data/fashion/train-labels-idx1-ubyte.gz')\n",
    "test_images_path = keras.utils.get_file('t10k-images-idx3-ubyte.gz', 'https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/data/fashion/t10k-images-idx3-ubyte.gz')\n",
    "test_labels_path = keras.utils.get_file('t10k-labels-idx1-ubyte.gz', 'https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/data/fashion/t10k-labels-idx1-ubyte.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist(images_path, labels_path):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_orig, y_train_orig = load_mnist(train_images_path, train_labels_path)\n",
    "X_test, y_test = load_mnist(test_images_path, test_labels_path)\n",
    "X_train_orig = X_train_orig.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train_orig /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_orig.shape)\n",
    "print(y_train_orig.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_orig, y_train_orig, test_size=0.2, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(48000,)\n",
      "(12000, 784)\n",
      "(12000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a367c5240>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nT+QUw0/kFMNP5BTDT+TU/wOP4iVkOh85ugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181c555f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[1, :].reshape((28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_px = X_train.mean().astype(np.float32)\n",
    "std_px = X_train.std().astype(np.float32)\n",
    "def norm_input(x): return (x-mean_px)/std_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn3 = Sequential([\n",
    "    Lambda(norm_input, input_shape=(28,28, 1)),\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu'),    \n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu'),    \n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    \n",
    "    Conv2D(128, kernel_size=(3, 3), activation='relu'),    \n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn3.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 596s 12ms/step - loss: 0.7053 - acc: 0.7588 - val_loss: 0.3926 - val_acc: 0.8568\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 301s 6ms/step - loss: 0.4238 - acc: 0.8485 - val_loss: 0.3397 - val_acc: 0.8799\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 281s 6ms/step - loss: 0.3511 - acc: 0.8749 - val_loss: 0.2686 - val_acc: 0.9046\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 283s 6ms/step - loss: 0.3084 - acc: 0.8900 - val_loss: 0.2565 - val_acc: 0.9095\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 285s 6ms/step - loss: 0.2787 - acc: 0.8994 - val_loss: 0.2315 - val_acc: 0.9169\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 285s 6ms/step - loss: 0.2596 - acc: 0.9058 - val_loss: 0.2315 - val_acc: 0.9164\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 284s 6ms/step - loss: 0.2404 - acc: 0.9124 - val_loss: 0.2074 - val_acc: 0.9254\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 342s 7ms/step - loss: 0.2197 - acc: 0.9205 - val_loss: 0.2208 - val_acc: 0.9237\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 321s 7ms/step - loss: 0.2133 - acc: 0.9229 - val_loss: 0.2016 - val_acc: 0.9293\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 285s 6ms/step - loss: 0.1997 - acc: 0.9271 - val_loss: 0.1925 - val_acc: 0.9319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a314f9358>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn3.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn3.optimizer.lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 276s 6ms/step - loss: 0.1930 - acc: 0.9304 - val_loss: 0.1978 - val_acc: 0.9316\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 278s 6ms/step - loss: 0.1798 - acc: 0.9354 - val_loss: 0.1923 - val_acc: 0.9343\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 327s 7ms/step - loss: 0.1696 - acc: 0.9394 - val_loss: 0.2083 - val_acc: 0.9282\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 323s 7ms/step - loss: 0.1655 - acc: 0.9396 - val_loss: 0.2069 - val_acc: 0.9291\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 290s 6ms/step - loss: 0.1540 - acc: 0.9442 - val_loss: 0.2033 - val_acc: 0.9312\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 286s 6ms/step - loss: 0.1442 - acc: 0.9471 - val_loss: 0.1822 - val_acc: 0.9372\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 291s 6ms/step - loss: 0.1400 - acc: 0.9490 - val_loss: 0.1897 - val_acc: 0.9384\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 327s 7ms/step - loss: 0.1304 - acc: 0.9514 - val_loss: 0.2018 - val_acc: 0.9329\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 387s 8ms/step - loss: 0.1260 - acc: 0.9536 - val_loss: 0.2016 - val_acc: 0.9323\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 387s 8ms/step - loss: 0.1167 - acc: 0.9569 - val_loss: 0.1920 - val_acc: 0.9398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a32ff7ac8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn3.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.21401405726000666\n",
      "Test accuracy: 0.9325\n"
     ]
    }
   ],
   "source": [
    "score = cnn3.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n",
    "                               height_shift_range=0.08, zoom_range=0.08)\n",
    "batches = gen.flow(X_train, y_train, batch_size=batch_size)\n",
    "val_batches = gen.flow(X_val, y_val, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "61/93 [==================>...........] - ETA: 1:38 - loss: 0.9933 - acc: 0.6626"
     ]
    }
   ],
   "source": [
    "cnn3.fit_generator(batches, steps_per_epoch=48000//batch_size, epochs=50,\n",
    "                    validation_data=val_batches, validation_steps=12000//batch_size, use_multiprocessing=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = cnn3.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
